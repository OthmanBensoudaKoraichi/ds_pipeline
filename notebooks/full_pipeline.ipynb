{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd15bf23",
   "metadata": {},
   "source": [
    "# 0 - Setup & imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a77553a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Add project root\n",
    "project_root = Path(\"..\").resolve()\n",
    "sys.path.append(str(project_root))\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afba7214",
   "metadata": {},
   "source": [
    "# Context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a102ac2",
   "metadata": {},
   "source": [
    "## Why an EDS alumnus made this workshop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ce81f1",
   "metadata": {},
   "source": [
    "With the rapid adoption of AI tools, two professional profiles are increasingly valuable.\n",
    "\n",
    "The first is the extreme specialist or researcher:\n",
    "\n",
    "- Works at the frontier of knowledge\n",
    "\n",
    "- Solves problems where AI lacks sufficient depth or domain understanding\n",
    "\n",
    "- Pushes into highly technical, unresolved areas\n",
    "\n",
    "The second is the generalist ‚Äî the role you are training for in this course:\n",
    "\n",
    "- Does not need to know every concept in detail\n",
    "\n",
    "- Needs to know what exists and how systems connect\n",
    "\n",
    "- Uses AI as a tool to explore, clarify, and reason through unfamiliar components\n",
    "\n",
    "In practice, success as a generalist means:\n",
    "\n",
    "- Understanding the full pipeline of a problem\n",
    "\n",
    "- Asking the right questions\n",
    "\n",
    "- Iterating with AI to deepen understanding where needed, on the job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43cf127a",
   "metadata": {},
   "source": [
    "## Job Market"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41a703b",
   "metadata": {},
   "source": [
    "![job_description](../images/job_description.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d5695e",
   "metadata": {},
   "source": [
    "## Real pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbbe1c5",
   "metadata": {},
   "source": [
    "![court](../images/court.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8835d37e",
   "metadata": {},
   "source": [
    "## It's not you, it's everybody "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49d35e8",
   "metadata": {},
   "source": [
    "![container](../images/container.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2840c79",
   "metadata": {},
   "source": [
    "# Full Pipeline "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc0aaa2",
   "metadata": {},
   "source": [
    "- Problem framing : Define the business question, target variable, constraints, and success metric.\n",
    "\n",
    "- Data collection : Pull data from databases, APIs, logs, files, or third-party sources.\n",
    "\n",
    "- Data understanding & EDA : Inspect distributions, missing values, bias, leakage, and sanity-check assumptions.\n",
    "\n",
    "- Data cleaning & feature engineering : Handle missing values, encode variables, create meaningful features, align schemas.\n",
    "\n",
    "- Train / validation / test split : Split data in a way that matches production reality (time-based if needed).\n",
    "\n",
    "- Modeling : Train baseline ‚Üí advanced models, tune hyperparameters, compare approaches.\n",
    "\n",
    "- Evaluation : Use the right metrics (ROC-AUC, F1, precision/recall, etc.), error analysis.\n",
    "\n",
    "- Experiment tracking : Log parameters, metrics, artifacts (e.g., with MLflow).\n",
    "\n",
    "- Packaging : Save the model, dependencies, and environment (often via Docker).\n",
    "\n",
    "- Deployment : Expose the model via API, batch job, or app (cloud or on-prem).\n",
    "\n",
    "- Monitoring & retraining : Track performance, drift, failures; retrain when data or behavior changes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d53102",
   "metadata": {},
   "source": [
    "# Building an Early-Warning System for Riverbend School District"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5aaf2a9",
   "metadata": {},
   "source": [
    "![High school](../images/high_school.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5cbcb06",
   "metadata": {},
   "source": [
    "# 1 - Scoping Phase ‚Äî Understanding the Human Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d9f092",
   "metadata": {},
   "source": [
    "As a data science consultant, you are responsible for helping the Riverbend School District address a growing concern around student disengagement and dropout risk. You will work directly with Dr. Aris Thorne, the district superintendent and your primary point of contact, to understand how the district defines success, risk, and failure. Your task is to translate this institutional knowledge into a complete, end-to-end data science solution ‚Äî from problem scoping and data exploration to modeling, evaluation, and deployment ‚Äî resulting in an early-warning system that can be used by district leadership to intervene before it is too late."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8522ea",
   "metadata": {},
   "source": [
    "## üìñ The Situation on the Ground\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a36e109",
   "metadata": {},
   "source": [
    "The Riverbend School District has experienced growing concern about student dropout and has asked an external data science consultancy team to investigate.\n",
    "\n",
    "Historically, the district operated under a traditional educational model:\n",
    "\n",
    "- In-person learning with strict physical attendance requirements\n",
    "\n",
    "- Grades recorded on a 5-point scale\n",
    "\n",
    "Under this system, district leadership believed that:\n",
    "\n",
    "- Physical absence was a strong warning signal\n",
    "\n",
    "- Declining grades reliably indicated academic risk\n",
    "\n",
    "- Student disengagement was relatively easy to detect\n",
    "\n",
    "Despite this, the district has observed an increase in dropout cases and growing uncertainty about when and how students disengage. As a result, leadership has turned to data science students to help design an early-warning system capable of identifying at-risk students before intervention becomes impossible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7198ac17",
   "metadata": {},
   "source": [
    "## üîÑ Project Lumina: A System in Transition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdb5104",
   "metadata": {},
   "source": [
    "Beginning in Semester 3, Riverbend launched Project Lumina, a district-wide reform that fundamentally altered how learning takes place.\n",
    "\n",
    "Under this new model:\n",
    "\n",
    "- Students can attend class physically or digitally, video lectures replace some in-person sessions, and academic engagement is no longer tied to daily physical presence\n",
    "\n",
    "- Grades are now recorded on a 20-point international scale\n",
    "\n",
    "From an administrative perspective, the system still looks the same ‚Äî students, courses, grades, attendance. But behavior no longer means what it used to mean."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f573ba20",
   "metadata": {},
   "source": [
    "## üéØ Objective\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3aaa38b",
   "metadata": {},
   "source": [
    "- The district has hired you to design and deliver an early-warning system that identifies students at risk of dropping out early enough to enable meaningful intervention."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78bcd847",
   "metadata": {},
   "source": [
    "## ‚ö†Ô∏è Important Constraint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63c2f3c",
   "metadata": {},
   "source": [
    "It is a real life project. Thus, at the start of the project:\n",
    "\n",
    "- There is no predefined target variable\n",
    "\n",
    "- There is no data dictionary\n",
    "\n",
    "- There is no explicit definition of ‚Äúdropout‚Äù in the data\n",
    "\n",
    "\n",
    "In practice:\n",
    "\n",
    "- Models are not built first\n",
    "\n",
    "- Understanding the institution comes before modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e26b433",
   "metadata": {},
   "source": [
    "## üß† Key Questions Before Writing Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e07e972",
   "metadata": {},
   "source": [
    "Before touching the data, you must answer:\n",
    "\n",
    "- How does the district define a dropout? Has this definition changed over time?\n",
    "\n",
    "- Which behaviors stopped meaning the same thing after Project Lumina?\n",
    "\n",
    "- What events could affect your model?\n",
    "\n",
    "If these questions are not addressed:\n",
    "\n",
    "- The model may perform well on paper but fail in real-world deployment\n",
    "\n",
    "Ask questions to Dr.Thorne, the superintendent : https://riverbend.streamlit.app/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5e7844",
   "metadata": {},
   "source": [
    "# 2 - Pull Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64daa699",
   "metadata": {},
   "source": [
    "- In real-world data science projects, data is rarely provided as clean CSV files. Instead, it often lives inside operational systems such as databases, internal tools, APIs, or even collections of PDFs. Accessing this data is part of the problem-solving process.\n",
    "\n",
    "- For Riverbend School District, student data is stored in a PostgreSQL database. We will use psycopg2, a standard Python adapter for PostgreSQL, to connect directly to the database and execute SQL queries. Psycopg2 is widely used in production because it is reliable and closely aligned with PostgreSQL‚Äôs native behavior.\n",
    "\n",
    "- Query results will then be loaded into pandas DataFrames, allowing us to combine SQL-based data extraction with Python-based exploration and modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdb3857",
   "metadata": {},
   "source": [
    "It is good practice to ask for the database schema. Some databases can be incredibly complex, and there is, more often than not, no data dictionary.\n",
    "This is usually a second part of the **scoping phase* , in which you choose the variables that you will pull. You need to ask lots of questions about the problem, the metrics you will need, and about the variables as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467f1bda",
   "metadata": {},
   "source": [
    "\n",
    "![schema](../images/schema.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8a49a8",
   "metadata": {},
   "source": [
    "Before diving into code, even at the exploration phase, we want to have a look at our problem, our schema, and ask ourself a few questions.\n",
    "\n",
    "- Based on our objective, what metrics do we want to use? Accuracy?\n",
    "\n",
    "- Which variables appear informative but are now misleading if used na√Øvely?\n",
    "\n",
    "- What drifts can influence the model?\n",
    "\n",
    "- Does data imbalance matter?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ec0e36",
   "metadata": {},
   "source": [
    "## Example pull"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40dbecbc",
   "metadata": {},
   "source": [
    "Here is a simple example with the students table. You will recognize SQL syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16193770",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k7/b0_b7t6j6n72t68sh4s7t8400000gn/T/ipykernel_7653/2445650149.py:15: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  student_id  birth_year  gender race_ethnicity parent_income_bracket  \\\n",
      "0     S00000        2009    male          white                   low   \n",
      "1     S00001        2010    male          white          upper_middle   \n",
      "2     S00002        2008  female          white                   low   \n",
      "3     S00003        2010    male          black          lower_middle   \n",
      "4     S00004        2010    male          white                  high   \n",
      "\n",
      "  parent_education_level  household_size primary_language  special_needs_flag  \\\n",
      "0                     hs               6          english               False   \n",
      "1                  no_hs               6          english               False   \n",
      "2               graduate               5            other               False   \n",
      "3                     hs               4          english               False   \n",
      "4                college               6          english               False   \n",
      "\n",
      "  school_id  \n",
      "0    SCH_05  \n",
      "1    SCH_00  \n",
      "2    SCH_01  \n",
      "3    SCH_03  \n",
      "4    SCH_03  \n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "\n",
    "# Connect to Supabase Postgres\n",
    "conn = psycopg2.connect(\n",
    "    host=\"aws-1-us-west-1.pooler.supabase.com\",\n",
    "    port=5432,\n",
    "    database=\"postgres\",\n",
    "    user=\"postgres.zzeuaztvmbhfednagzlz\",\n",
    "    password=\"edsdataset2026\",\n",
    "    sslmode=\"require\"\n",
    ")\n",
    "\n",
    "# Simple test query with the students tables\n",
    "df = pd.read_sql(\n",
    "    \"SELECT * FROM students LIMIT 5;\",\n",
    "    conn\n",
    ")\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81fd5c4",
   "metadata": {},
   "source": [
    "## Try to pull data yourself, after understanding variables and knowing what you want to pull, what joins you want to make etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd068f47",
   "metadata": {},
   "source": [
    "**Note:** In practice, it is rare to pull entire database tables into a local environment. This step is included here for transparency and exploration. In real-world projects, data scientists typically prepare SQL queries that extract only the specific columns, time periods, and records needed for the task at hand, both for efficiency and data governance reasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae334e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pulling table: students\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k7/b0_b7t6j6n72t68sh4s7t8400000gn/T/ipykernel_7653/3659775494.py:47: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 5950 rows to ../data/csv_backup/students.csv\n",
      "Pulling table: schools\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k7/b0_b7t6j6n72t68sh4s7t8400000gn/T/ipykernel_7653/3659775494.py:47: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 10 rows to ../data/csv_backup/schools.csv\n",
      "Pulling table: courses\n",
      "Saved 4 rows to ../data/csv_backup/courses.csv\n",
      "Pulling table: course_grades\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k7/b0_b7t6j6n72t68sh4s7t8400000gn/T/ipykernel_7653/3659775494.py:47: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n",
      "/var/folders/k7/b0_b7t6j6n72t68sh4s7t8400000gn/T/ipykernel_7653/3659775494.py:47: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 55016 rows to ../data/csv_backup/course_grades.csv\n",
      "Pulling table: attendance_records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k7/b0_b7t6j6n72t68sh4s7t8400000gn/T/ipykernel_7653/3659775494.py:47: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 275080 rows to ../data/csv_backup/attendance_records.csv\n",
      "Pulling table: learning_activity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k7/b0_b7t6j6n72t68sh4s7t8400000gn/T/ipykernel_7653/3659775494.py:47: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 17534 rows to ../data/csv_backup/learning_activity.csv\n",
      "Pulling table: disciplinary_events\n",
      "Saved 134 rows to ../data/csv_backup/disciplinary_events.csv\n",
      "‚úÖ All tables successfully backed up to CSV.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k7/b0_b7t6j6n72t68sh4s7t8400000gn/T/ipykernel_7653/3659775494.py:47: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# CONFIG\n",
    "# ---------------------------------------------------------\n",
    "OUTPUT_DIR = \"../data/csv_files\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# CONNECT TO SUPABASE POSTGRES\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "conn = psycopg2.connect(\n",
    "    host=\"aws-1-us-west-1.pooler.supabase.com\",\n",
    "    port=5432,\n",
    "    database=\"postgres\",\n",
    "    user=\"postgres.zzeuaztvmbhfednagzlz\",\n",
    "    password=\"edsdataset2026\",\n",
    "    sslmode=\"require\"\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# TABLES TO BACK UP\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "tables = [\n",
    "    \"students\",\n",
    "    \"schools\",\n",
    "    \"courses\",\n",
    "    \"course_grades\",\n",
    "    \"attendance_records\",\n",
    "    \"learning_activity\",\n",
    "    \"disciplinary_events\"\n",
    "]\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# PULL + SAVE EACH TABLE\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "for table in tables:\n",
    "    print(f\"Pulling table: {table}\")\n",
    "    \n",
    "    query = f\"SELECT * FROM {table};\"\n",
    "    df = pd.read_sql(query, conn)\n",
    "    \n",
    "    output_path = os.path.join(OUTPUT_DIR, f\"{table}.csv\")\n",
    "    df.to_csv(output_path, index=False)\n",
    "    \n",
    "    print(f\"Saved {len(df)} rows to {output_path}\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# CLEAN UP\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "conn.close()\n",
    "print(\"‚úÖ All tables successfully backed up to CSV.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c82ac0",
   "metadata": {},
   "source": [
    "# Explore your data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe612b2",
   "metadata": {},
   "source": [
    "## üîç Exploratory Data Analysis (EDA)\n",
    "\n",
    "Once the data is loaded and joined, the next step is **exploration**.  \n",
    "This phase is not about modeling yet ‚Äî it‚Äôs about **understanding what you are working with**.\n",
    "\n",
    "### 1Ô∏è‚É£ Look at your columns\n",
    "\n",
    "Start by listing and scanning all columns:\n",
    "- What does each column represent?\n",
    "- Which ones are numerical, categorical, dates, IDs, or text?\n",
    "- Are there columns that look redundant, constant, or suspicious?\n",
    "\n",
    "This step often reveals:\n",
    "- Unexpected columns\n",
    "- Naming inconsistencies\n",
    "- Encoded information that needs clarification\n",
    "\n",
    "\n",
    "\n",
    "### 2Ô∏è‚É£ Inspect distributions\n",
    "\n",
    "For each column, you should ask:\n",
    "- What are the typical values?\n",
    "- Are there outliers or extreme values?\n",
    "- Are there missing values, and how frequent are they?\n",
    "- Do some columns have skewed or multi-modal distributions?\n",
    "\n",
    "Understanding distributions helps you:\n",
    "- Decide on transformations\n",
    "- Detect data quality issues\n",
    "- Anticipate modeling challenges\n",
    "\n",
    "\n",
    "\n",
    "### 3Ô∏è‚É£ Understand what the columns *really* mean  \n",
    "(scoping continues here)\n",
    "\n",
    "Exploration is also part of the **scoping phase**.\n",
    "\n",
    "At this stage, you are not just looking at numbers ‚Äî you are trying to understand:\n",
    "- How the data was generated\n",
    "- Which columns are inputs vs outcomes\n",
    "- Which variables are actionable, noisy, or proxy signals\n",
    "\n",
    "Very often:\n",
    "- There is no data dictionary\n",
    "- Column definitions are incomplete or ambiguous\n",
    "- The only way forward is to **ask questions** to someone who knows the domain\n",
    "\n",
    "This is realistic and expected in real-world projects.\n",
    "\n",
    "\n",
    "\n",
    "### 4Ô∏è‚É£ Manual vs automated exploration\n",
    "\n",
    "You can explore the data in two complementary ways:\n",
    "\n",
    "**Manual exploration**\n",
    "- `df.head()`, `df.describe()`, `df.value_counts()`\n",
    "- Custom plots and sanity checks\n",
    "- Slower, but forces deep understanding\n",
    "\n",
    "**Automated exploration tools**\n",
    "- Tools like **YData Profiling** generate an HTML report with:\n",
    "  - Column summaries\n",
    "  - Distributions\n",
    "  - Missing value analysis\n",
    "  - Correlations\n",
    "- Faster and great for first-pass diagnostics\n",
    "\n",
    "Both approaches are useful:\n",
    "- Automated tools give you breadth\n",
    "- Manual exploration gives you depth\n",
    "\n",
    "The goal of EDA is not to be exhaustive, but to reach a point where the data *makes sense* and the next modeling decisions are justified.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdd5c59",
   "metadata": {},
   "source": [
    "## Example with ydataprofiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61504be6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/othmanbensouda/Desktop/eds_workshop/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 1864.76it/s]?, ?it/s, Describe variable: core_subject]\n",
      "Summarize dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [00:00<00:00, 22.84it/s, Completed]                 \n",
      "Generate report structure: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.74it/s]\n",
      "Render HTML: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.04it/s]\n",
      "Export report to file: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 1219.27it/s]\n"
     ]
    }
   ],
   "source": [
    "from ydata_profiling import ProfileReport\n",
    "import pandas as pd\n",
    "df = # THE DF YOU WANT TO GENERATE A REPORT ABOUT\n",
    "profile = ProfileReport(df, title=\"EDA Report\")\n",
    "profile.to_file(\"report.html\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6416d0bd",
   "metadata": {},
   "source": [
    "Ydata profiling will generate a report.html, that you can open with a browser like Google Chrome. Try it out, it's pretty cool and very fast."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10edded",
   "metadata": {},
   "source": [
    "# Modeling (Train/Test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79145fc0",
   "metadata": {},
   "source": [
    "Now, you can start modeling. Ask yourself what features you need to train, what you need to test, and how you would define a good model.\n",
    "Here is a quick hint and reminder regarding metrics : "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a102b3b4",
   "metadata": {},
   "source": [
    " ![precision_recall](../images/precision_recall.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f84969c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TEST PERFORMANCE (S1 FEATURES ‚Üí S2 DROPOUT) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97      4126\n",
      "           1       0.74      0.70      0.72       419\n",
      "\n",
      "    accuracy                           0.95      4545\n",
      "   macro avg       0.85      0.84      0.84      4545\n",
      "weighted avg       0.95      0.95      0.95      4545\n",
      "\n",
      "ROC AUC: 0.979\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "\n",
    "# =========================================================\n",
    "# LOAD DATA (RELATIVE PATH)\n",
    "# =========================================================\n",
    "\n",
    "DATASET_PATH = \"../data/csv_files\"\n",
    "\n",
    "students = pd.read_csv(f\"{DATASET_PATH}/students.csv\")\n",
    "attendance = pd.read_csv(f\"{DATASET_PATH}/attendance_records.csv\")\n",
    "grades = pd.read_csv(f\"{DATASET_PATH}/course_grades.csv\")\n",
    "labels = pd.read_csv(f\"{DATASET_PATH}/dropout_labels.csv\")\n",
    "\n",
    "# =========================================================\n",
    "# S1 FEATURES (TRAIN)\n",
    "# =========================================================\n",
    "\n",
    "att_s1 = attendance[attendance[\"academic_year\"] == \"2024_S1\"]\n",
    "grades_s1 = grades[grades[\"academic_year\"] == \"2024_S1\"]\n",
    "\n",
    "att_feat_s1 = (\n",
    "    att_s1.groupby(\"student_id\")\n",
    "    .agg(\n",
    "        mean_absent=(\"days_absent\", \"mean\"),\n",
    "        total_absent=(\"days_absent\", \"sum\"),\n",
    "        max_absent_week=(\"days_absent\", \"max\")\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "grade_feat_s1 = (\n",
    "    grades_s1.groupby(\"student_id\")\n",
    "    .agg(\n",
    "        mean_grade=(\"grade_value\", \"mean\"),\n",
    "        min_grade=(\"grade_value\", \"min\")\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "labels_s2 = labels[labels[\"semester\"] == \"2024_S2\"][[\"student_id\", \"dropout\"]]\n",
    "\n",
    "train_df = (\n",
    "    students\n",
    "    .merge(att_feat_s1, on=\"student_id\", how=\"inner\")\n",
    "    .merge(grade_feat_s1, on=\"student_id\", how=\"inner\")\n",
    "    .merge(labels_s2, on=\"student_id\", how=\"inner\")\n",
    ")\n",
    "\n",
    "X_train = train_df.drop(columns=[\"student_id\", \"dropout\"])\n",
    "y_train = train_df[\"dropout\"]\n",
    "\n",
    "# =========================================================\n",
    "# PREPROCESSING\n",
    "# =========================================================\n",
    "\n",
    "num_cols = X_train.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
    "cat_cols = X_train.select_dtypes(include=[\"object\", \"bool\"]).columns.tolist()\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(), num_cols),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False), cat_cols),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# =========================================================\n",
    "# MODEL\n",
    "# =========================================================\n",
    "\n",
    "model = GradientBoostingClassifier(\n",
    "    n_estimators=300,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=3,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"prep\", preprocessor),\n",
    "    (\"clf\", model)\n",
    "])\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "# =========================================================\n",
    "# S2 FEATURES (TEST)\n",
    "# =========================================================\n",
    "\n",
    "s1_students = set(att_feat_s1.student_id)\n",
    "\n",
    "att_s2 = attendance[\n",
    "    (attendance[\"academic_year\"] == \"2024_S2\") &\n",
    "    (attendance[\"student_id\"].isin(s1_students))\n",
    "]\n",
    "\n",
    "grades_s2 = grades[\n",
    "    (grades[\"academic_year\"] == \"2024_S2\") &\n",
    "    (grades[\"student_id\"].isin(s1_students))\n",
    "]\n",
    "\n",
    "att_feat_s2 = (\n",
    "    att_s2.groupby(\"student_id\")\n",
    "    .agg(\n",
    "        mean_absent=(\"days_absent\", \"mean\"),\n",
    "        total_absent=(\"days_absent\", \"sum\"),\n",
    "        max_absent_week=(\"days_absent\", \"max\")\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "grade_feat_s2 = (\n",
    "    grades_s2.groupby(\"student_id\")\n",
    "    .agg(\n",
    "        mean_grade=(\"grade_value\", \"mean\"),\n",
    "        min_grade=(\"grade_value\", \"min\")\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "test_df = (\n",
    "    students\n",
    "    .merge(att_feat_s2, on=\"student_id\", how=\"inner\")\n",
    "    .merge(grade_feat_s2, on=\"student_id\", how=\"inner\")\n",
    "    .merge(labels_s2, on=\"student_id\", how=\"inner\")\n",
    ")\n",
    "\n",
    "X_test = test_df.drop(columns=[\"student_id\", \"dropout\"])\n",
    "y_test = test_df[\"dropout\"]\n",
    "\n",
    "# =========================================================\n",
    "# EVALUATION\n",
    "# =========================================================\n",
    "\n",
    "y_pred = pipe.predict(X_test)\n",
    "y_prob = pipe.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"\\n=== TEST PERFORMANCE (S1 FEATURES ‚Üí S2 DROPOUT) ===\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"ROC AUC:\", round(roc_auc_score(y_test, y_prob), 3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c038a02",
   "metadata": {},
   "source": [
    "## Experimenting "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c10d47",
   "metadata": {},
   "source": [
    "In practice, model development is rarely a single run. Data scientists typically run many experiments to test different feature sets, hyperparameters, and modeling choices, while systematically tracking their results for comparison and reproducibility. \n",
    "\n",
    "A common tool to do this is MLflow, which also allows you to deploy your model and make it usable by your client."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a294dfb0",
   "metadata": {},
   "source": [
    " ![mlflow](../images/mlflow_experiments.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d039b782",
   "metadata": {},
   "source": [
    "# Create an API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ad35e0",
   "metadata": {},
   "source": [
    "In practice, data science does not live in notebooks.\n",
    "\n",
    "Models and decision logic are used by other systems: dashboards, internal tools, and applications.\n",
    "An API is how those systems communicate with your logic. People might not ask for your code directly, but will want to interact with your model/app.\n",
    "\n",
    "By creating an API, you:\n",
    "\n",
    "- separate decision logic from the interface\n",
    "\n",
    "- make your work reusable and deployable\n",
    "\n",
    "- move from analysis to a real system\n",
    "\n",
    "This is how models are used in production.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0941e1d0",
   "metadata": {},
   "source": [
    "From the project root, run *uvicorn src.flask_app:app --reload*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041ffb64",
   "metadata": {},
   "source": [
    "If successful, you will see : Uvicorn running on http://127.0.0.1:8000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576c68fd",
   "metadata": {},
   "source": [
    "# Use your API and build a simple User interface to interact with it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378783a5",
   "metadata": {},
   "source": [
    "An API by itself is not useful to most users.\n",
    "\n",
    "In practice, decision-makers interact with models through simple interfaces, not code.\n",
    "The role of the user interface is to collect inputs, send them to the API, and display the result in a clear and actionable way.\n",
    "\n",
    "In this project, you will use Streamlit to build a minimal interface that communicates with your API, demonstrating how data science systems are used by real people outside notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bd8302",
   "metadata": {},
   "source": [
    "Run in your terminal : *streamlit run src/streamlit_app.py*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4cc73ef",
   "metadata": {},
   "source": [
    "# Containerize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6b1d42",
   "metadata": {},
   "source": [
    "When code runs on your laptop, it depends on:\n",
    "\n",
    "- your operating system\n",
    "\n",
    "- your Python version\n",
    "\n",
    "- your installed libraries\n",
    "\n",
    "This is why ‚Äúit works on my machine‚Äù is such a common failure.\n",
    "\n",
    "Containerization solves this by packaging:\n",
    "\n",
    "- the code\n",
    "\n",
    "- the dependencies\n",
    "\n",
    "- the runtime environment\n",
    "\n",
    "into a single, portable unit called a container.\n",
    "\n",
    "In practice, this means:\n",
    "\n",
    "- the API runs the same way everywhere\n",
    "\n",
    "- deployment is predictable\n",
    "\n",
    "- debugging environment issues disappears\n",
    "\n",
    "This is how models are deployed in real systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6f145c",
   "metadata": {},
   "source": [
    "Dockerhub is like Github for containers. There are many : Google Artifact Registry, Azure Artifact Registry etc.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359049a8",
   "metadata": {},
   "source": [
    "![dockerhub](../images/dockerhub.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4c03ab",
   "metadata": {},
   "source": [
    "Once you build your dockerfile, you can build your docker image and put it on registries like dockerhub. From the project root, write *docker build -t riverbend-app .*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660e4975",
   "metadata": {},
   "source": [
    "# Requirements file (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e1ea7f",
   "metadata": {},
   "source": [
    "Tip : to get a requirements.txt file with all libraries, use pip freeze > requirements.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
